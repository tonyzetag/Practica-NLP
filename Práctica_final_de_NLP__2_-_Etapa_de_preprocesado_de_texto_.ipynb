{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbf9ca4",
   "metadata": {},
   "source": [
    "# 2.1.- Importamos nuestro Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c047df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c78fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga del dataset\n",
    "def get_json_data(file):\n",
    "    for line in open(file):\n",
    "        yield json.loads(line)\n",
    "\n",
    "# Los n=10 primeros\n",
    "corpus_path = \"./Corpus/\"\n",
    "file1 = corpus_path + \"reviews_Home_and_Kitchen_5.json\"\n",
    "file2 = corpus_path + \"reviews_Tools_and_Home_Improvement_5.json\"\n",
    "n_samples = 100000\n",
    "\n",
    "df = pd.concat([pd.DataFrame(itertools.islice(get_json_data(file1),n_samples)), \n",
    "                pd.DataFrame(itertools.islice(get_json_data(file2),n_samples))],\n",
    "               axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2afb3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Price</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>10 19, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>zoku</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>06 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>05 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>08 4, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>PugLover</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "      <td>1402099200</td>\n",
       "      <td>06 7, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin            reviewerName   helpful  \\\n",
       "0   APYOBQE6M18AA  0615391206         Martin Schwartz    [0, 0]   \n",
       "1  A1JVQTAGHYOL7F  0615391206           Michelle Dinh    [0, 0]   \n",
       "2  A3UPYGJKZ0XTU4  0615391206            mirasreviews  [26, 27]   \n",
       "3  A2MHCTX43MIMDZ  0615391206  M. Johnson \"Tea Lover\"  [14, 18]   \n",
       "4   AHAI85T5C2DH3  0615391206                PugLover    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  My daughter wanted this book and the price on ...      5.0   \n",
       "1  I bought this zoku quick pop for my daughterr ...      5.0   \n",
       "2  There is no shortage of pop recipes available ...      4.0   \n",
       "3  This book is a must have if you get a Zoku (wh...      5.0   \n",
       "4  This cookbook is great.  I have really enjoyed...      4.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Best Price      1382140800   \n",
       "1                                               zoku      1403049600   \n",
       "2  Excels at Sweet Dessert Pops, but Falls Short ...      1367712000   \n",
       "3                                    Creative Combos      1312416000   \n",
       "4            A must own if you own the Zoku maker...      1402099200   \n",
       "\n",
       "    reviewTime  \n",
       "0  10 19, 2013  \n",
       "1  06 18, 2014  \n",
       "2   05 5, 2013  \n",
       "3   08 4, 2011  \n",
       "4   06 7, 2014  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testeo\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397c69e",
   "metadata": {},
   "source": [
    "# 2.2.- Preparación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76f917",
   "metadata": {},
   "source": [
    "Lo primero vamos a quedarnos con las columnas que nos valen para nuestro análisis del sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c899944",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"reviewerID\", \"asin\", \"reviewerName\", \"helpful\", \"unixReviewTime\", \"reviewTime\", \"summary\"]\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e850dc2",
   "metadata": {},
   "source": [
    "Ahora vamos a definir nuestro criterio de sentimiento positivo y sentimiento negativo, tal como lo hicimos en el primer notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ee01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(row):\n",
    "    return 0 if int(row[['overall']]) < 4 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c572f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df.apply(lambda row: sentiment(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f76172f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall  sentiment\n",
       "0  My daughter wanted this book and the price on ...      5.0          1\n",
       "1  I bought this zoku quick pop for my daughterr ...      5.0          1\n",
       "2  There is no shortage of pop recipes available ...      4.0          1\n",
       "3  This book is a must have if you get a Zoku (wh...      5.0          1\n",
       "4  This cookbook is great.  I have really enjoyed...      4.0          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542bdb8",
   "metadata": {},
   "source": [
    "# 2.3.- Separación en conjuntos de train - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f1c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49792939",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['reviewText'],\n",
    "    df['sentiment'],\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    random_state=52,\n",
    "    shuffle=True,\n",
    "    stratify=df['sentiment']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe896f45",
   "metadata": {},
   "source": [
    "# 2.4.- Definición del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621d53cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tonyz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tonyz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tonyz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías\n",
    "import nltk\n",
    "\n",
    "# Descargas de nltk necesarias\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de65bd",
   "metadata": {},
   "source": [
    "Vamos a añadir todas las funciones ya vistas en el apartado 1 que va a formar nuestro **pipeline** para el **preprocesado del texto**, como variante se va a intentar dar uso a **generadores** para que no sea tan pesado en memoria (sobretodo en test indivduales donde usaré pocas reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e16888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizado\n",
    "def tokenizer_word_level(reviews, tokenizer=nltk.RegexpTokenizer(r'\\w+')):\n",
    "    for review in reviews:\n",
    "        yield tokenizer.tokenize(review)\n",
    "\n",
    "# A minúsculas\n",
    "def corpus_to_lower(reviews):\n",
    "    for review in reviews:\n",
    "        yield [word.lower() for word in review]\n",
    "\n",
    "# Stopwords\n",
    "def drop_stopwords(reviews, stopwords=nltk.corpus.stopwords.words('english')):\n",
    "    for review in reviews:\n",
    "        yield [word for word in review if word not in stopwords]\n",
    "        \n",
    "# Stemming\n",
    "def stemming_data(reviews, stemmer=nltk.stem.snowball.EnglishStemmer(ignore_stopwords=True)):\n",
    "    for review in reviews:\n",
    "        yield [stemmer.stem(word) for word in review]  \n",
    "        \n",
    "# Lemmatizer\n",
    "def lemmatizing_data(reviews, lemmatizer=nltk.stem.WordNetLemmatizer()):\n",
    "    for review in reviews:\n",
    "        yield [lemmatizer.lemmatize(word) for word in review]\n",
    "        \n",
    "# Limpiado de no-alphas\n",
    "def cleaning_no_alpha(reviews):\n",
    "    for review in reviews:\n",
    "        yield [word for word in review if str.isalpha(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f178392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_generator(reviews, to_lower=True, drop_sw=True, stemming=True, lemmatizing=False, only_alpha=True,\n",
    "                      stopwords=nltk.corpus.stopwords.words('english'), lemmatizer=nltk.stem.WordNetLemmatizer(),\n",
    "                      stemmer=nltk.stem.snowball.EnglishStemmer(ignore_stopwords=True)):\n",
    "    \n",
    "    # Tokenizamos\n",
    "    corpus = tokenizer_word_level(reviews)\n",
    "    \n",
    "    # Pasamos a minusculas\n",
    "    corpus = corpus_to_lower(corpus) if to_lower else corpus\n",
    "    \n",
    "    # Quitamos stopwords\n",
    "    corpus = drop_stopwords(corpus, stopwords=stopwords) if drop_sw else corpus\n",
    "    \n",
    "    # Realizamos Stemming\n",
    "    corpus = stemming_data(corpus, stemmer=stemmer) if stemming else corpus\n",
    "    \n",
    "    # Realizamos Lemmatizing\n",
    "    corpus = lemmatizing_data(corpus, lemmatizer=lemmatizer) if lemmatizing else corpus\n",
    "    \n",
    "    # Limpiamos las palabras que no sean enteras alphabéticas\n",
    "    corpus = cleaning_no_alpha(corpus) if only_alpha else corpus\n",
    "    \n",
    "    # Devolvemos nuestro corpus como generador\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f6ec3",
   "metadata": {},
   "source": [
    "Nota: Para el punto 1 usé lemmatización, ya que a la hora de visualizar vemos las palabras enteras y creo que es mejor para la parte de exploración, en este apartado vamos a usar stemming, que en el caso del Inglés aporta mas valor que la lemmatización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03ca27",
   "metadata": {},
   "source": [
    "# 2.5.- Limpieza y guardado de variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26e00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que estamos en analisis de sentimientos, voy a hacer un limpiado de stopwords muy light, ya que es sensible\n",
    "my_stopwords = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'the', 'or', 'and', 'is', 'are', 'but', 'a', 'at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47dbe0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64935     This is so cute. I love it. The antenna are ju...\n",
      "36086     This is very good product, very easy to clean ...\n",
      "16787     This is a popcorn lover's delight.  I could no...\n",
      "158933    I am crazy do-it-yourself person around the ho...\n",
      "114806    It does what it supposed to do. I  moved to an...\n",
      "                                ...                        \n",
      "69310     After purchasing a cheap supermarket store ove...\n",
      "115490    Like the header says this is a must have tool ...\n",
      "168503    I'm not sure there is a huge difference betwee...\n",
      "116351    I've owned or used a lot of cordless drills, f...\n",
      "136387    A bungee is a bungee, so check the price. If t...\n",
      "Name: reviewText, Length: 150000, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Primero vamos a ver de que tipo es X_train e Y_train antes de modifcarlo\n",
    "print(X_train)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "562fb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que tenemos pandas.core.series.Series, voy a realizar el pipeline solo al texto \n",
    "# sin modificar la estructura para evitar futuros problemas\n",
    "\n",
    "for i, review in enumerate(X_train):\n",
    "    X_train.iloc[i] = \" \".join(next(pipeline_generator([review], stopwords = my_stopwords)))\n",
    "    \n",
    "for i, review in enumerate(X_test):\n",
    "    X_test.iloc[i] = \" \".join(next(pipeline_generator([review], stopwords = my_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61f6dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64935     this so cute love antenna just wire stick out ...\n",
      "36086     this very good product very easi to clean good...\n",
      "16787     this popcorn lover s delight could not be happ...\n",
      "158933    am crazi do yourself person around hous am alw...\n",
      "114806    does what suppos to do move to an apart where ...\n",
      "                                ...                        \n",
      "69310     after purchas cheap supermarket store oven the...\n",
      "115490    like header say this must have tool if instal ...\n",
      "168503    m not sure there huge differ between all wirel...\n",
      "116351    ve own use lot of cordless drill from black de...\n",
      "136387    bunge bunge so check price if this price suita...\n",
      "Name: reviewText, Length: 150000, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos\n",
    "print(X_train)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc2fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos\n",
    "! mkdir Vars\n",
    "X_train.to_pickle(\"./Vars/X_train.pkl\")\n",
    "X_test.to_pickle(\"./Vars/X_test.pkl\")\n",
    "y_train.to_pickle(\"./Vars/y_train.pkl\")\n",
    "y_test.to_pickle(\"./Vars/y_test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
