{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4bc847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ya existe el subdirectorio o el archivo Models.\n",
      "Ya existe el subdirectorio o el archivo Score.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Extracción de características\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Procesado\n",
    "import sklearn.preprocessing as pr\n",
    "import multiprocessing as mp\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "# Configuración\n",
    "train_model_1 = True\n",
    "train_model_2 = True\n",
    "train_model_3_w2v = True\n",
    "train_model_3_lstm = True\n",
    "\n",
    "!mkdir Models\n",
    "!mkdir Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4139ec8d",
   "metadata": {},
   "source": [
    "# 3.1.- Carga de datos y preparación del validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8f3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f54ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Vars/X_train.pkl', 'rb') as file:\n",
    "    X = pickle.load(file)\n",
    "    \n",
    "with open('./Vars/y_train.pkl', 'rb') as file:\n",
    "    y = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab41b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.80,\n",
    "    test_size=0.20,\n",
    "    random_state=52,\n",
    "    shuffle=True,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d7b92",
   "metadata": {},
   "source": [
    "# 3.2.- Modelo 1 - Extracción de características TF-IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b588b2",
   "metadata": {},
   "source": [
    "La idea es usar este modelo de Machine Learning para ver como se comportan los datos, habiendo previamente preprocesado los datos (tokenizado, limpieza, normalizado...) y preparando una extracción de características TF-IDF para alimentar a dicho modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c8c8f",
   "metadata": {},
   "source": [
    "## 3.2.1.- Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04f2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "    def __init__(self, ngram_range, strip_accents, max_df, min_df, max_features):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            strip_accents=strip_accents,\n",
    "            max_df=max_df,\n",
    "            min_df=min_df,\n",
    "            max_features=max_features)\n",
    "         \n",
    "    def fit(self, train_data):\n",
    "        self.vectorizer.fit(train_data)\n",
    "        \n",
    "    def transform(self, data):\n",
    "        return self.vectorizer.transform(data)\n",
    "        \n",
    "    def get_vocabulary(self):\n",
    "        return self.vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b6c45",
   "metadata": {},
   "source": [
    "## 3.2.2.- Procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5cb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos objeto\n",
    "tf_idf = TF_IDF(ngram_range=(1, 3),\n",
    "                strip_accents='ascii',\n",
    "                max_df=0.95,\n",
    "                min_df=3,\n",
    "                max_features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869ff083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "tf_idf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e139e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "X_train_f = tf_idf.transform(X_train)\n",
    "X_valid_f = tf_idf.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07aca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulario\n",
    "vocab = tf_idf.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e5c85",
   "metadata": {},
   "source": [
    "## 3.2.3.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c45e17e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Accuracy for C=0.01: 0.8378\n",
      "Val_Accuracy for C=0.05: 0.8587666666666667\n",
      "Val_Accuracy for C=0.25: 0.8866333333333334\n",
      "Val_Accuracy for C=0.5: 0.8922333333333333\n",
      "Val_Accuracy for C=1: 0.8953666666666666\n",
      "Val_Accuracy for C=10: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Accuracy for C=100: 0.8952666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Accuracy for C=1000: 0.8951666666666667\n",
      "Val_Accuracy for C=10000: 0.8951666666666667\n",
      "Best model\n",
      "\tc: 10\n",
      "\ttrain acc: 0.9088833333333334\n",
      "\tval acc: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "if train_model_1:\n",
    "\n",
    "    # Regularización\n",
    "    c_params = [0.01, 0.05, 0.25, 0.5, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "    # Métricas\n",
    "    train_acc = list()\n",
    "    valid_acc = list()\n",
    "    best_c = 0\n",
    "    best_train_acc = 0\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for c in c_params:\n",
    "        # Creo modelo y lo entreno\n",
    "        model = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
    "        model.fit(X_train_f, y_train)\n",
    "        \n",
    "        # Predicción del modelo\n",
    "        y_train_pred = model.predict(X_train_f)\n",
    "        y_valid_pred = model.predict(X_valid_f)\n",
    "\n",
    "        # Guardo métricas\n",
    "        train_acc.append(accuracy_score(y_train, y_train_pred))\n",
    "        valid_acc.append(accuracy_score(y_valid, y_valid_pred))\n",
    "        print (\"Val_Accuracy for C={}: {}\".format(c, accuracy_score(y_valid, y_valid_pred)))\n",
    "\n",
    "        # Checkpoint (Guardo siempre el modelo de mayor accuracy en val)\n",
    "        if max(valid_acc) == accuracy_score(y_valid, y_valid_pred):\n",
    "            pickle.dump(model, open(\"./Models/model_1.pkl\", 'wb'))\n",
    "            best_c = c\n",
    "            best_train_acc = accuracy_score(y_train, y_train_pred)\n",
    "            best_valid_acc = accuracy_score(y_valid, y_valid_pred)\n",
    "\n",
    "    print(\"Best model\\n\\tc: {}\\n\\ttrain acc: {}\\n\\tval acc: {}\".format(best_c, best_train_acc, best_valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e8f4f",
   "metadata": {},
   "source": [
    "## 3.2.5.- Guardando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dbc0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el modelo\n",
    "with open('./Models/model_1.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb43e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el error\n",
    "error_train_params = (y_train, model.predict(X_train_f))\n",
    "error_valid_params = (y_valid, model.predict(X_valid_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3817144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "scores = {\n",
    "    \"Name\": \"TF-IDF + Logistic Regression\",\n",
    "    \"Confusion Matrix train\": confusion_matrix(*error_train_params),\n",
    "    \"Report train\": classification_report(*error_train_params),\n",
    "    \"Accuracy train\": accuracy_score(*error_train_params),\n",
    "    \"Confusion Matrix val\": confusion_matrix(*error_valid_params),\n",
    "    \"Report val\": classification_report(*error_valid_params),\n",
    "    \"Accuracy val\": accuracy_score(*error_valid_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91f681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: TF-IDF + Logistic Regression\n",
      "Accuracy val data: 0.8968\n"
     ]
    }
   ],
   "source": [
    "print(\"Name: {}\\nAccuracy val data: {}\".format(scores[\"Name\"], scores[\"Accuracy val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a3ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardo\n",
    "pickle.dump(scores, open(\"./Score/model_1_scores.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d4c11",
   "metadata": {},
   "source": [
    "# 3.3.- Modelo 2 - Extracción de características Bag_of_Words + LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e1e59",
   "metadata": {},
   "source": [
    "## 3.3.1.- Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb36a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoW:\n",
    "    def __init__(self, max_features):\n",
    "        self.vectorizer = CountVectorizer(max_features=max_features)\n",
    "         \n",
    "    def fit(self, train_data):\n",
    "        self.vectorizer.fit(train_data)\n",
    "        \n",
    "    def transform(self, data, normalize=False):\n",
    "        if normalize:\n",
    "            return  pr.normalize(self.vectorizer.transform(data), axis=1)\n",
    "        else:\n",
    "            return self.vectorizer.transform(data)\n",
    "        \n",
    "    def get_vocabulary(self):\n",
    "        return self.vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d01ce9",
   "metadata": {},
   "source": [
    "## 3.3.2.- Procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a425b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos objeto\n",
    "bow = BoW(max_features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c483492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "bow.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f1e927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform (y normaliza)\n",
    "X_train_f = bow.transform(X_train, normalize=True)\n",
    "X_valid_f = bow.transform(X_valid, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6a3385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulario\n",
    "vocab = bow.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5ccc3",
   "metadata": {},
   "source": [
    "## 3.3.3.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a895f3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Accuracy for C=0.01: 0.8388333333333333\n",
      "Val_Accuracy for C=0.05: 0.8554333333333334\n",
      "Val_Accuracy for C=0.25: 0.8747\n",
      "Val_Accuracy for C=0.5: 0.8805666666666667\n",
      "Val_Accuracy for C=1: 0.8839333333333333\n",
      "Val_Accuracy for C=10: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Accuracy for C=100: 0.8856666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Accuracy for C=1000: 0.8851\n",
      "Val_Accuracy for C=10000: 0.8852\n",
      "Best model\n",
      "\tc: 10\n",
      "\ttrain acc: 0.8977333333333334\n",
      "\tval acc: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "if train_model_2:\n",
    "\n",
    "    # Regularización\n",
    "    c_params = [0.01, 0.05, 0.25, 0.5, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "    # Métricas\n",
    "    train_acc = list()\n",
    "    valid_acc = list()\n",
    "    best_c = 0\n",
    "    best_train_acc = 0\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for c in c_params:\n",
    "        # Creo modelo y lo entreno\n",
    "        model = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
    "        model.fit(X_train_f, y_train)\n",
    "        \n",
    "        # Predicción del modelo\n",
    "        y_train_pred = model.predict(X_train_f)\n",
    "        y_valid_pred = model.predict(X_valid_f)\n",
    "\n",
    "        # Guardo métricas\n",
    "        train_acc.append(accuracy_score(y_train, y_train_pred))\n",
    "        valid_acc.append(accuracy_score(y_valid, y_valid_pred))\n",
    "        print (\"Val_Accuracy for C={}: {}\".format(c, accuracy_score(y_valid, y_valid_pred)))\n",
    "\n",
    "        # Checkpoint (Guardo siempre el modelo de mayor accuracy en val)\n",
    "        if max(valid_acc) == accuracy_score(y_valid, y_valid_pred):\n",
    "            pickle.dump(model, open(\"./Models/model_2.pkl\", 'wb'))\n",
    "            best_c = c\n",
    "            best_train_acc = accuracy_score(y_train, y_train_pred)\n",
    "            best_valid_acc = accuracy_score(y_valid, y_valid_pred)\n",
    "\n",
    "    print(\"Best model\\n\\tc: {}\\n\\ttrain acc: {}\\n\\tval acc: {}\".format(best_c, best_train_acc, best_valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509947a",
   "metadata": {},
   "source": [
    "## 3.3.4.- Guardando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee50a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el modelo\n",
    "with open('./Models/model_2.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e65a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el error\n",
    "error_train_params = (y_train, model.predict(X_train_f))\n",
    "error_valid_params = (y_valid, model.predict(X_valid_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd27b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "scores = {\n",
    "    \"Name\": \"BoW + Logistic Regression\",\n",
    "    \"Confusion Matrix train\": confusion_matrix(*error_train_params),\n",
    "    \"Report train\": classification_report(*error_train_params),\n",
    "    \"Accuracy train\": accuracy_score(*error_train_params),\n",
    "    \"Confusion Matrix val\": confusion_matrix(*error_valid_params),\n",
    "    \"Report val\": classification_report(*error_valid_params),\n",
    "    \"Accuracy val\": accuracy_score(*error_valid_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4ffc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardo\n",
    "pickle.dump(scores, open(\"./Score/model_2_scores.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96913b84",
   "metadata": {},
   "source": [
    "# 3.4.- Modelo 3 - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313d8f9",
   "metadata": {},
   "source": [
    "## 3.4.1.- Vectorización (Word-Embeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "142271ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        cum_loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, cum_loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, cum_loss - self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "580b7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V:\n",
    "    def __init__(self, vector_size, window, min_count, sg, hs, negative, workers, seed):\n",
    "        self.vector_size = vector_size\n",
    "        self.w2v = Word2Vec(vector_size=vector_size,\n",
    "                            window=window,\n",
    "                            min_count=min_count,\n",
    "                            sg=sg,\n",
    "                            hs=hs,\n",
    "                            negative=negative,\n",
    "                            workers=workers,\n",
    "                            seed=seed)\n",
    "        \n",
    "    def build_vocabulary(self, data):\n",
    "        self.w2v.build_vocab(self.buid_corpus(data))\n",
    "        \n",
    "    def update_vocabulary(self, data):\n",
    "        self.w2v.build_vocab(self.buid_corpus(data), update=True)\n",
    "        \n",
    "    def buid_corpus(self, train_data):\n",
    "        return [review.split() for review in train_data]\n",
    "         \n",
    "    def fit(self, train_data, epochs=2):\n",
    "        # Construyo el vocabulario\n",
    "        self.w2v.build_vocab(self.buid_corpus(train_data))\n",
    "        \n",
    "        # train (llamamos función para crear el corpus y callback personalizado)\n",
    "        self.w2v.train(self.buid_corpus(train_data), \n",
    "                       total_examples=self.w2v.corpus_count, \n",
    "                       epochs=epochs, \n",
    "                       compute_loss=True, \n",
    "                       callbacks=[callback()])\n",
    "        \n",
    "    def transform(self, data):\n",
    "        word2id = self.w2v.wv.key_to_index\n",
    "        id2word = {i: word for word, i in word2id.items()}\n",
    "        # padding y vectorizado\n",
    "        data = [[word2id.get(word) for word in review.split()] for review in data]\n",
    "        return sequence.pad_sequences(data, maxlen=self.vector_size)\n",
    "        \n",
    "    def get_vocabulary(self):\n",
    "        return self.w2v.wv.key_to_index\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a92ae13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = int(np.quantile([len(words.split()) for words in X_train], 0.95))\n",
    "cbow_params = {\n",
    "            'vector_size': max_words,\n",
    "            'window': 10,\n",
    "            'min_count': 1,\n",
    "            'sg': 0,\n",
    "            'hs': 0,\n",
    "            'negative': 20,\n",
    "            'workers' : 8,\n",
    "            'seed' : 52\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f85bec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = W2V(**cbow_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e20f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 3383253.0\n",
      "Loss after epoch 1: 2729093.0\n",
      "Loss after epoch 2: 2592010.0\n",
      "Loss after epoch 3: 2409132.0\n",
      "Loss after epoch 4: 2346413.0\n"
     ]
    }
   ],
   "source": [
    "w2v.fit(X_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64e2783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.update_vocabulary(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1d7bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f = w2v.transform(X_train)\n",
    "X_valid_f = w2v.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28606610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     9,   421,  2147],\n",
       "       [    0,     0,     0, ...,    39,    21,    81],\n",
       "       [    0,     0,     0, ...,     8,  6965,  1802],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    17,   256,    80],\n",
       "       [  108,     0, 21646, ...,     3,  2544,     6],\n",
       "       [    0,     0,     0, ...,   206,  2363,   422]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5169a55",
   "metadata": {},
   "source": [
    "## 3.4.2.- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a5c955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para la capa embeding\n",
    "w2v_weights = w2v.get_weights()\n",
    "vocab_size, emdedding_size = w2v_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5015e9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53652, 236)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec6672e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, RepeatVector\n",
    "\n",
    "with tf.device('gpu:0'):\n",
    "    # Sequential\n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, \n",
    "                        weights=[w2v_weights]))\n",
    "    # LSTM + Dropout\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e4f3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding False\n",
      "lstm True\n",
      "dropout True\n",
      "dense True\n",
      "dropout_1 True\n",
      "dense_1 True\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False\n",
    "for l in model.layers:\n",
    "    print(l.name, l.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2d200d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 236)         12661872  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               504832    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,232,753\n",
      "Trainable params: 570,881\n",
      "Non-trainable params: 12,661,872\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3ade58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 81s 163ms/step - loss: 0.3240 - accuracy: 0.8652 - val_loss: 0.2832 - val_accuracy: 0.8834\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 76s 161ms/step - loss: 0.2548 - accuracy: 0.8963 - val_loss: 0.2548 - val_accuracy: 0.8974\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 76s 162ms/step - loss: 0.2242 - accuracy: 0.9088 - val_loss: 0.2484 - val_accuracy: 0.8975\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 76s 162ms/step - loss: 0.2004 - accuracy: 0.9200 - val_loss: 0.2441 - val_accuracy: 0.9022\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 76s 161ms/step - loss: 0.1762 - accuracy: 0.9295 - val_loss: 0.2439 - val_accuracy: 0.9038\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 74s 157ms/step - loss: 0.1520 - accuracy: 0.9398 - val_loss: 0.2580 - val_accuracy: 0.9058\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 72s 154ms/step - loss: 0.1282 - accuracy: 0.9489 - val_loss: 0.2706 - val_accuracy: 0.9046\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 72s 155ms/step - loss: 0.1036 - accuracy: 0.9592 - val_loss: 0.3103 - val_accuracy: 0.9010\n"
     ]
    }
   ],
   "source": [
    "if train_model_3_lstm:\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "    batch_size = 256\n",
    "    num_epochs = 10\n",
    "\n",
    "    model.fit(X_train_f, y_train,\n",
    "              validation_data=(X_valid_f, y_valid),\n",
    "              batch_size=batch_size, epochs=num_epochs,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=3, restore_best_weights=True),\n",
    "                         ModelCheckpoint(\"./Models/model_3.h5\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22180b25",
   "metadata": {},
   "source": [
    "## 3.4.3.- Guardamos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71794186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos los resultados al mismo formato que y\n",
    "def prepare_lstm_output(y_pred):\n",
    "    return np.array(np.round(y_pred), dtype=np.int64).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a4dae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Cargo el modelo\n",
    "model = load_model(\"./Models/model_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e01d5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el error\n",
    "error_train_params = (y_train, prepare_lstm_output(model.predict(X_train_f)))\n",
    "error_valid_params = (y_valid, prepare_lstm_output(model.predict(X_valid_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0771c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "scores = {\n",
    "    \"Name\": \"Word2Vec + LSTM\",\n",
    "    \"Confusion Matrix train\": confusion_matrix(*error_train_params),\n",
    "    \"Report train\": classification_report(*error_train_params),\n",
    "    \"Accuracy train\": accuracy_score(*error_train_params),\n",
    "    \"Confusion Matrix val\": confusion_matrix(*error_valid_params),\n",
    "    \"Report val\": classification_report(*error_valid_params),\n",
    "    \"Accuracy val\": accuracy_score(*error_valid_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03b73e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardo\n",
    "pickle.dump(scores, open(\"./Score/model_3_scores.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b2c91",
   "metadata": {},
   "source": [
    "# 3.5.- Predicción en conjunto de test\n",
    "Llegamos a la parte final, vamos a ver como se comportan nuestros modelos entrenados con el conjunto de test para posteriormente evaluarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a297ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Vars/X_train.pkl', 'rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "    \n",
    "with open('./Vars/y_train.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "    \n",
    "with open('./Vars/X_test.pkl', 'rb') as file:\n",
    "    X_test = pickle.load(file)\n",
    "    \n",
    "with open('./Vars/y_test.pkl', 'rb') as file:\n",
    "    y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a13288b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuelve el score previamente guardado\n",
    "def score_from_pkl(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb330514",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"./Score/model_1_scores.pkl\", \n",
    "         \"./Score/model_2_scores.pkl\", \n",
    "         \"./Score/model_3_scores.pkl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fee863",
   "metadata": {},
   "source": [
    "## 3.5.1.- Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5095f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el modelo\n",
    "with open('./Models/model_1.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6bfe611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formateo entrada\n",
    "X_test_f = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75153430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el error\n",
    "error_test_params = (y_test, model.predict(X_test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a38fe8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo scores\n",
    "scores = score_from_pkl(paths[0])\n",
    "# Añado las nuevas scores\n",
    "scores.update({\n",
    "    \"Confusion Matrix test\": confusion_matrix(*error_test_params),\n",
    "    \"Report test\": classification_report(*error_test_params),\n",
    "    \"Accuracy test\": accuracy_score(*error_test_params)\n",
    "})\n",
    "# Guado scores\n",
    "pickle.dump(scores, open(paths[0], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8002a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_1:\n",
    "    # guardo\n",
    "    pickle.dump(scores, open(\"./Score/model_1_scores.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c513d",
   "metadata": {},
   "source": [
    "## 3.5.2.- Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d5288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el modelo\n",
    "with open('./Models/model_2.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a517ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formateo entrada\n",
    "X_test_f = bow.transform(X_test, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a74af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el error\n",
    "error_test_params = (y_test, model.predict(X_test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2aad5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo scores\n",
    "scores = score_from_pkl(paths[1])\n",
    "# Añado las nuevas scores\n",
    "scores.update({\n",
    "    \"Confusion Matrix test\": confusion_matrix(*error_test_params),\n",
    "    \"Report test\": classification_report(*error_test_params),\n",
    "    \"Accuracy test\": accuracy_score(*error_test_params)\n",
    "})\n",
    "# Guado scores\n",
    "pickle.dump(scores, open(paths[1], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63747392",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_1:\n",
    "    # guardo\n",
    "    pickle.dump(scores, open(\"./Score/model_2_scores.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087abc4",
   "metadata": {},
   "source": [
    "## 3.5.3.- Modelo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef6a4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el modelo\n",
    "model = load_model(\"./Models/model_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05482969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formateo entrada\n",
    "w2v.update_vocabulary(X_test)\n",
    "X_test_f = w2v.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38f6c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el error\n",
    "error_test_params = (y_test, prepare_lstm_output(model.predict(X_test_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6771a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo scores\n",
    "scores = score_from_pkl(paths[2])\n",
    "# Añado las nuevas scores\n",
    "scores.update({\n",
    "    \"Confusion Matrix test\": confusion_matrix(*error_test_params),\n",
    "    \"Report test\": classification_report(*error_test_params),\n",
    "    \"Accuracy test\": accuracy_score(*error_test_params)\n",
    "})\n",
    "# Guado scores\n",
    "pickle.dump(scores, open(paths[2], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "139c08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_1:\n",
    "    # guardo\n",
    "    pickle.dump(scores, open(\"./Score/model_3_scores.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83242b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
